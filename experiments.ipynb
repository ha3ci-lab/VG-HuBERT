{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./vg-hubert_3\"\n",
    "wav_file = \"/home/ldap-users/Share/Data/librispeech/train-clean-100/19/198/19-198-0037.flac\"\n",
    "tgt_layer = 9\n",
    "threshold = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring trm.cls_token due to not existing or size mismatch\n",
      "Ignoring trm.pos_embed due to not existing or size mismatch\n",
      "Ignoring trm.patch_embed.proj.weight due to not existing or size mismatch\n",
      "Ignoring trm.patch_embed.proj.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.0.norm1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.0.norm1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.0.attn.qkv.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.0.attn.qkv.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.0.attn.proj.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.0.attn.proj.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.0.norm2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.0.norm2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.0.mlp.fc1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.0.mlp.fc1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.0.mlp.fc2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.0.mlp.fc2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.1.norm1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.1.norm1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.1.attn.qkv.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.1.attn.qkv.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.1.attn.proj.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.1.attn.proj.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.1.norm2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.1.norm2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.1.mlp.fc1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.1.mlp.fc1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.1.mlp.fc2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.1.mlp.fc2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.2.norm1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.2.norm1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.2.attn.qkv.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.2.attn.qkv.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.2.attn.proj.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.2.attn.proj.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.2.norm2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.2.norm2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.2.mlp.fc1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.2.mlp.fc1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.2.mlp.fc2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.2.mlp.fc2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.3.norm1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.3.norm1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.3.attn.qkv.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.3.attn.qkv.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.3.attn.proj.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.3.attn.proj.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.3.norm2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.3.norm2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.3.mlp.fc1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.3.mlp.fc1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.3.mlp.fc2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.3.mlp.fc2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.4.norm1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.4.norm1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.4.attn.qkv.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.4.attn.qkv.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.4.attn.proj.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.4.attn.proj.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.4.norm2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.4.norm2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.4.mlp.fc1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.4.mlp.fc1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.4.mlp.fc2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.4.mlp.fc2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.5.norm1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.5.norm1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.5.attn.qkv.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.5.attn.qkv.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.5.attn.proj.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.5.attn.proj.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.5.norm2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.5.norm2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.5.mlp.fc1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.5.mlp.fc1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.5.mlp.fc2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.5.mlp.fc2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.6.norm1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.6.norm1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.6.attn.qkv.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.6.attn.qkv.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.6.attn.proj.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.6.attn.proj.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.6.norm2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.6.norm2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.6.mlp.fc1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.6.mlp.fc1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.6.mlp.fc2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.6.mlp.fc2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.7.norm1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.7.norm1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.7.attn.qkv.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.7.attn.qkv.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.7.attn.proj.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.7.attn.proj.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.7.norm2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.7.norm2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.7.mlp.fc1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.7.mlp.fc1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.7.mlp.fc2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.7.mlp.fc2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.8.norm1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.8.norm1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.8.attn.qkv.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.8.attn.qkv.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.8.attn.proj.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.8.attn.proj.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.8.norm2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.8.norm2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.8.mlp.fc1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.8.mlp.fc1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.8.mlp.fc2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.8.mlp.fc2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.9.norm1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.9.norm1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.9.attn.qkv.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.9.attn.qkv.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.9.attn.proj.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.9.attn.proj.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.9.norm2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.9.norm2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.9.mlp.fc1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.9.mlp.fc1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.9.mlp.fc2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.9.mlp.fc2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.10.norm1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.10.norm1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.10.attn.qkv.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.10.attn.qkv.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.10.attn.proj.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.10.attn.proj.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.10.norm2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.10.norm2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.10.mlp.fc1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.10.mlp.fc1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.10.mlp.fc2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.10.mlp.fc2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.11.norm1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.11.norm1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.11.attn.qkv.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.11.attn.qkv.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.11.attn.proj.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.11.attn.proj.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.11.norm2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.11.norm2.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.11.mlp.fc1.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.11.mlp.fc1.bias due to not existing or size mismatch\n",
      "Ignoring trm.blocks.11.mlp.fc2.weight due to not existing or size mismatch\n",
      "Ignoring trm.blocks.11.mlp.fc2.bias due to not existing or size mismatch\n",
      "Ignoring trm.norm.weight due to not existing or size mismatch\n",
      "Ignoring trm.norm.bias due to not existing or size mismatch\n",
      "Ignoring audio_cls_token_proj.0.weight due to not existing or size mismatch\n",
      "Ignoring audio_cls_token_proj.0.bias due to not existing or size mismatch\n",
      "Ignoring audio_cls_token_proj.2.weight due to not existing or size mismatch\n",
      "Ignoring audio_cls_token_proj.2.bias due to not existing or size mismatch\n",
      "Ignoring visual_cls_token_proj.0.weight due to not existing or size mismatch\n",
      "Ignoring visual_cls_token_proj.0.bias due to not existing or size mismatch\n",
      "Ignoring visual_cls_token_proj.2.weight due to not existing or size mismatch\n",
      "Ignoring visual_cls_token_proj.2.bias due to not existing or size mismatch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import soundfile as sf\n",
    "import os\n",
    "import pickle\n",
    "from models import audio_encoder\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "def cls_attn_seg(cls_attn_weights, threshold, spf, audio_len_in_sec):\n",
    "\n",
    "    threshold_value = torch.quantile(cls_attn_weights, threshold, dim=-1, keepdim=True) # [n_h, T]\n",
    "    boundary_idx = torch.where((cls_attn_weights >= threshold_value).float().sum(0) > 0)[0].cpu().numpy()\n",
    "\n",
    "    word_boundaries_list = []\n",
    "    word_boundary_intervals = []\n",
    "    attn_boundary_intervals = []\n",
    "\n",
    "    for k, g in groupby(enumerate(boundary_idx), lambda ix : ix[0] - ix[1]):\n",
    "        seg = list(map(itemgetter(1), g))\n",
    "        t_s, t_e = seg[0], min(seg[-1]+1, cls_attn_weights.shape[-1])\n",
    "        if len(seg) > 1:\n",
    "            attn_boundary_intervals.append([spf*t_s, spf*t_e])\n",
    "\n",
    "    for left, right in zip(attn_boundary_intervals[:-1], attn_boundary_intervals[1:]):\n",
    "        word_boundaries_list.append((left[1]+right[0])/2.)\n",
    "    \n",
    "    for i in range(len(word_boundaries_list)-1):\n",
    "        word_boundary_intervals.append([word_boundaries_list[i], word_boundaries_list[i+1]])\n",
    "    return {\"attn_boundary_intervals\": attn_boundary_intervals, \"word_boundary_intervals\": word_boundary_intervals}\n",
    "\n",
    "# setup model\n",
    "with open(os.path.join(model_path, \"args.pkl\"), \"rb\") as f:\n",
    "    model_args = pickle.load(f)\n",
    "model = audio_encoder.AudioEncoder(model_args)\n",
    "bundle = torch.load(os.path.join(model_path, \"best_bundle.pth\"))\n",
    "model.carefully_load_state_dict(bundle['dual_encoder'], load_all=True)\n",
    "model.eval()\n",
    "model = model.cuda()\n",
    "\n",
    "# load waveform (do not layer normalize the waveform!)\n",
    "audio, sr = sf.read(wav_file, dtype = 'float32')\n",
    "assert sr == 16000\n",
    "audio_len_in_sec = len(audio) / sr\n",
    "audio = torch.from_numpy(audio).unsqueeze(0).cuda() # [T] -> [1, T]\n",
    "\n",
    "# model forward\n",
    "with torch.no_grad():\n",
    "    model_out = model(audio, padding_mask=None, mask=False, need_attention_weights=True, tgt_layer=tgt_layer)\n",
    "feats = model_out['features'].squeeze(0)[1:] # [1, T+1, D] -> [T, D]\n",
    "spf = audio.shape[-1]/sr/feats.shape[-2]\n",
    "attn_weights = model_out['attn_weights'].squeeze(0) # [1, num_heads, T+1, T+1] -> [num_heads, T+1, T+1] (for the two T+1, first is target length then the source)\n",
    "cls_attn_weights = attn_weights[:, 0, 1:] # [num_heads, T+1, T+1] -> [num_heads, T]\n",
    "out = cls_attn_seg(cls_attn_weights, threshold, spf, audio_len_in_sec) # out contains attn boundaries and word boundaries in intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11     [[0.2009478672985782, 0.36170616113744075], [0.38180094786729857, 0.5425592417061611], [0.6832227488151659, 0.8640758293838863], [0.9042654028436019, 1.0650236966824644], [1.1654976303317535, 1.4870142180094787], [2.4917535545023695, 2.6324170616113745], [2.692701421800948, 3.014218009478673], [3.034312796208531, 3.235260663507109], [3.2754502369668246, 3.3759241706161136], [3.3960189573459716, 3.6974407582938387], [3.757725118483412, 3.9586729857819907]]\n",
      "9     [[0.3717535545023697, 0.6128909952606635], [0.6128909952606635, 0.8841706161137441], [0.8841706161137441, 1.115260663507109], [1.115260663507109, 1.989383886255924], [1.989383886255924, 2.662559241706161], [2.662559241706161, 3.024265402843602], [3.024265402843602, 3.2553554502369666], [3.2553554502369666, 3.385971563981043], [3.385971563981043, 3.7275829383886254]]\n"
     ]
    }
   ],
   "source": [
    "print(len(out[\"attn_boundary_intervals\"]), \"   \", out[\"attn_boundary_intervals\"])\n",
    "print(len(out[\"word_boundary_intervals\"]), \"   \", out[\"word_boundary_intervals\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
